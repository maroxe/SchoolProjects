%%% Local Variables:
%%% mode: latex
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-master: t
%%% End:

%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[utf8]{inputenc}  
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{tikz}
%\usepackage{minted}

%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}

%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule
\newcommand{\espr}[1]{
  \mathrm{E}^Q \left[ #1 \right]
}

\newcommand{\Qespr}[2]{
  \mathrm{E}^{#1} \left[ #2 \right]
}


\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{Ecole Polytechnique} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Modèles des Taux d'interêt \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont \normalsize
                Bachir EL KHADIR\\[-3pt] \normalsize
                \today	
}
\date{}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}

\newcommand{\IMG}[3]{
  \begin{center}
    \includegraphics[scale=#3]{#1}%
    \end{center}
}


%%% Begin document
\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\input{merci.tex}
\newpage
\input{intro.tex} 
\newpage
\input{partie1.tex} 
\newpage
\section{Les différents modèles des taux d'interêts}

Nous avons vu dans la partie précédente que pour trouver un prix aux dérivés de taux, il faut donner la dynamique qui régit le sous-jacent, dans notre cas  *BLA BLA BLA*
Plusieurs approches sont possibles. Nous pouvons modéliser directement le forward:

$$\mathrm{d} L = \sigma L^\beta \mathrm{d}W_t$$

L'aproche historique, décrit la dynamique du taux d'interêt instantané comme étant ``drivé'' par un driver à une seule dimension. C'est l'approche que nous adopterons ici.
Ceci est pratique dans le seul où les prix de zéro coupon et le taux sont directement disponible dans le modèle.
De plus, certains produits financiers dépendent directement la courbe de rendement.
Nous avons vu précédemment que la donnée du taux instantané $r_t$ permet de caractériser complètement cette courbe. 

Il est donc important que la dynamique de $r_t$ soit à la fois riche pour pouvoir décrire la courbe de rendement observée dans le marché, et suffisemment simple pour que le temps nécessaire pour le calcul ne soit trop long.

Nous pouvons considérer que la courbe de rendement varie dans un espace vectoriel de dimension infinie. Toute tentative de la caracteriser par un nombre fini de paramètre rééls est donc vouée à l'échec.

Le modèle de Hull White a été introduit en 1990.  Un des atouts majeure de ce modèle est la possiblité de simuler la dynamique de $r_t$ par un arbre trinomiale. Ceci étant essentiel pour pricer des produits du type bermuda options
$$ \mathrm{d}r_t =  (\theta(t) - \alpha(t) r_t) \mathrm{d}t + \sigma(t) \mathrm{d} W_t$$
$$ r_t = e^{-\alpha t} r_0 + integral ...$$


\newpage
\section{Le modèle à deux facteurs}
\subsection{Motivation}


Considérons un produit $E$ dont le payoff  dépent  du spread entre un taux d'intêret cumulé entre $0$ et $T_1$ pour le premier et $0$ et $T_2$ pour le second. $E$ dépend donc de la distribution jointe des deux taux.

La figure suivante, tiré du PDF *bla bla bla* reproduit une matrice de corrélation par terme de variations quotidiennes de taux zéro-coupon. Il apparait très clairement que des taux de maturités proches, comme le taux de maturité 3 ans et celui de maturité 4 ans, sont très corrélés, tandis que des taux de maturité éloignées (par exemple le taux 1 mois et le taux 10 ans) le sont très peu:

\IMG{img/tabcorr.png}{Tableau de correlation}{0.3}

Dans la réalité, on observe sur les marchés que les taux à différentes maturités ne sont pas corrélés. Si on regarde le taux 2Y et 10Y

\IMG{img/libor.png}{Libor}{1}

\subsubsection*{Limite des modèle à un seul facteur}
Montrons dans un premier temps pourquoi un modèle à un seul facteur n'est pas suffisant pour pricer ces produits qui dépendent non seulement de la distribution de chaque courbe de taux, mais aussi de leur corrélation. 

La dynamique de $r_t$ dans le modèle de Vasicek est donné par
$$r_t = k(\theta - r_t)  \mathrm{d}t  + \sigma \mathrm{d}W_t$$
La formule analytique du bond zéro coupon est donc
$$P(t, T) = A(t, T) exp(-B(t, T) r_t)$$
En particulier le taux d'intêret cumulé est une donné par une transformation affine du taux instantané:
$$R(t, T) = \frac{ln P(t, T)}{T-t} =: a(t, T) + b(t, T) r_t$$
Le payoff du produit $E$ est donc fonction de la distribution jointe de $R(0, T_1)$ et $R(0, T_2)$. Sauf que:
$$Cor(R(0, T_1), R(0, T_2) = 1$$

On en déduit qu'un choc à $r_t$ agit de la même manière sur toutes les courbes.


Un modèle à un seul facteur ne capture pas ce comportement. Essayons de pallier à ce problème en rajoutons un facteur à ce modèle.

Dans cette section nous considérons un modèle où le taux d'intêret instantanté est donné par une somme de deux facteurs gaussiens centrés et corrélés. Dans ce modèle doit sa popularité au fait que le prix des bond zéron coupon admet une formule exact, ainsi que le prix des caps et des floors.


\begin{align*}
  \rm{d}x &= -\alpha x(t) \rm{d}t + \sigma \rm{d}W^1_t \\
  \rm{d}y &= -\beta y(t) \rm{d}t + \nu \rm{d}W^2_t \\
  \rm{d}r &= x + y 
\end{align*}

\newpage
\section{Approximation de la solution par un arbre binomial}


L'équation (*) s'intègre simplement en:
$$x(t) = x(s) e^{-\alpha (t-s)} +  \sigma \int_s^t e^{- \alpha (t-u)} \rm{d} W^1_u $$
$$y(t) = x(s) e^{-\beta (t-s)} +  \nu \int_s^t e^{- \beta (t-u)} \rm{d} W^2_u $$

\subsubsection{Construction}

Cette méthode a été d'abord suggéré par Hull-White (1994)

On discrétise l’intervalle $[0, T]$ avec les temps $T_i = i \Delta t$, où $\Delta t = \frac{T}{N}$.
Nous donnons une approximation de la dynamique processus $x$ et $y$, par une suite de variable de discretisé $((\widetilde{x}_i, \widetilde{y}_i) \approx (x(i \Delta i), (y(i \Delta t))_i $. Pour celà nous calculerons les deux premier moment de $(x, y)$

$$E(x(t+\Delta t) | F_t) = x(t) e^{-a \Delta t}$$
$$V(x(t+\Delta t) | F_t) = \frac{\sigma^2}{2a} (1 - e^{-2a \Delta t})$$
$$Cov\{x(t+\Delta t), y(t+\Delta t) | F_t \} = \frac{\sigma \nu \rho}{a + b} (1-e^{-(a+b)\Delta t})$$

Pour que le $((\widetilde{x}_i, \widetilde{y}_i)$ et $(x(i \Delta i), (y(i \Delta t))_i $ aient les même moment, la loi de  $((\widetilde{x}_i, \widetilde{y}_i)$  est donnée par:
$$\mathrm{P} \left( \widetilde{x}_{i+1} = \widetilde{x}_i + a \, \mathrm{d}x, \widetilde{y}_{i+1} = \widetilde{y}_i + b \, \mathrm{d}y |  \widetilde{x}_i, \widetilde{y}_i \right) = p^{a, b}( \widetilde{x}_i, \widetilde{y}_i)$$
où 
\begin{itemize}
\item $a, b \in \{-1, +1\}$
\item $p$ est donnée par
\end{itemize}

Nous appelons slice l'ensemble des noeuds qui sont équi distant de la racine. Une slice représente la distribution du processus $( \widetilde{x}_i, \widetilde{y}_i)$ à un instant donné.

Le pricing se fait en deux temps:
\begin{itemize}
\item On diffuse le processus $(\widetilde{x}, \widetilde{y})$ dans l'arbre en prenant soin de calculer la probabilité de transition d'un état à un autre
\item On ``drawback'' dans l'arbre en partant de la date à laquelle on fait le payoff, en ***discountant***
\end{itemize}

  \subsubsection*{Petite discussion sur la courbe d'actualisation vs la courbe de diffusion}
  Avant la crise de 2008, il était d'usage courant que les banques considèrent le taux Libor comme reflétant la réalité du marché de crédit inter-bancaire. Le taux est publié quotidiennement par
taux réél auquel les banques sont prête à se prétter de l'argent est appelé taux OIS (Overnight Index Swap).  
Pendant la crise, le spread entre LIBOR et OIS était si grand qu'il devenait impossible à ignorer. Depuis tous les modèles de taux intégrent deux courbe, une pour la diffusion (LIBOR par exemple) et une autre pour l'actualisation (OIS).
Dans le développement de cet article, nous ignorons cette différence.
  
\subsection{Améliorations}
Si nous implémentons l'abre de façon naîve, le nombre de noeuds augmente de façon exponentielle en fonction du nombre de pas de temps. En pratique ceci est problématique et conduit vite à une saturation de mémoire. Dans l'exemple simplifié ci-dessus nous traçons l'arbre de diffusion du premier facteur ($x(t)$). A chaque pas de temps le nombre de noeuds double, ie pour $n$ pas de temps, nous nous retrouvons avec $2^n$ noeuds pour un facteur, ou $4^n$ pour deux. 

Remarquer que $(x, y)_i$tilde est un processus markovien homogène à valeurs discrètes nous permet d'optimiser la simulation de l'arbre. En effet, il nous suffit de calculer la table de transition une fois au début du programme et de la réutiliser pour avancer/reculer dans le temps. 

\input{graph.tex}


\IMG{img/slice.png}{Slice}{0.5}
\IMG{img/pending.jpg}{Cache grind avant}{0.2}
\IMG{img/pending.jpg}{Cache grind avant}{0.2}

Une autre améliroation possible est de trouver une formule analytique pour certain produits. En effet, si nous reprenons l'exemple d'un cancellable spread option 2Y10Y dont la maturité est dans 5 ans, nous devrions normalement construite l'arbre jusqu'en 2030 pour avoir le taux 10Y en 2020. Nous pouvons éviter celà en fournissant directement une formule exacte pour les zéro coupons.

\subsubsection{Formule exacte}
Cette partie est fortement insipiré de *BLA BLA BLA*

On rappelle l'expression du prix du bond zéron coupon sous la mesure risque neutre $Q$
$$P(t, T) = \espr{ e^{-\int_t^T r_u \rm{d}u}} $$

Notons $I(t, T) := \int_t^T x(u) + y(u) \rm{d}u$, et montrons que conditionnellement à l'information accumulé jusqu'au temps $t$, c'est une variable normale d'esperance $M(t, T)$ et de variance $V(t, T)$ où:
$$M(t, T) := ...$$
$$V(t, T) := ...$$

Nous avons donc
$$P(t, T) = exp  \left( \espr{ -\int_t^T r_u \rm{d}u} + \frac{1}{2} Var \left( {-\int_t^T r_u \rm{d}u} \right)  \right) $$

Nous utiliserons cette formule directement dans le pricer, ce qui nous évitera de construire l'arbre jusqu'à la date de maturité du dernier zéro coupon.

\subsubsection{Ajout d'un shift déterministe}
Le modèle, tel que développé jusqu'a présent, présente un inconvénient majeur: à tout instant, $r_t$ est symétriquement distribué autour de $0$. Ceci ne correspond pas à la réalité, puisque les taux négatifs ne sont observé dans les marchés que dans de très rares circonstance ( en Europe après crise inter-bancaire de 2009, Au Japon après des années de déflation ). Une autre raison est que le modèle ne permet pas de retrouver les prix des bonds zéro coupon.

Pour pallier à ce problème, nous rajoutons une fonction $\phi$ au taux $r_t$. La fonction déterministe $\phi(t)$ permet de fitter exactement la courbe de taux observée. Dans la partie ``Calibraion nous'' nous verrons comment calculer cette fonction à partir des prix de bonds zéro coupon.

En prenant en compte ce changement, le prix du bond zéro coupon devient:

$$P(t, T) = \espr{ e^{-r_t + \phi(t)}}$$

Nous prendrons soin de modifier l'étape de ``draw back'' dans l'abre en changeant le facteur d'actualisation.

Le processus est markovien

\subsubsection{Optimiser la taille des slices }
La taille de la slice augmente linéairement avec le temps. Ce n'est pas raisonnable.
Comment connecter les slices entre elles


\subsubsection{ Paramètres dépendant du temps}

Dans la partie précédente, les paramètres $\sigma \nu \alpha \beta$ étaient constantes. Considérer des variables qui dépendent  du temps permet au modèle plus de flexibilité pour fitter les données de marché. Voir la partie calibration.

Deux problèmes cependant:

\begin{itemize}
\item Le calcul est beaucoup plus long,
\item possibilité de sur fitter les données historique du marché, ce qui affecte négativement le pouvoir prédicitive du modèle
\end{itemize}

<Calcul>

On vérifie expérimentalement que le gain est significatif

\IMG{img/pending.jpg}{Cache grind avant}{0.2}
\IMG{img/pending.jpg}{Cache grind apres}{0.2}

\section{Résultats}

Une fois toutes ces modifications prises en compte, nous pouvons *BLA BLA BLA*
Paragraphe sur la taille des slice, ellipsoid
\IMG{img/pending.jpg}{Slice 2D}{0.2}


\begin{itemize}
\item Discrétisation
\item Construction de l'abre
\end{itemize}


\subsection{Performance}
L'arbre est long mais beaucoup plus puissant
Imperfections de l'arbre:
\begin{itemize}
\item  bornee
\item  discretisation
\item  probabilite negative
\end{itemize}
Monte Carlo
Limitation
Closed Form

Le temps d'execution:
Arbre
Closed form

\newpage

\section{Application: calibration et pricing}
Notre modèle possède à un certains nombres de paramètres libre que nous devons fixer. Pour celà, nous choisissons un certain nombre d'actifs tradables dans le marché, dont le prix est donc connus, que nous appelerons benchmark. Nous essayerons ensuite de trouver les paramètres qui reproduisent le mieux ces prix là. Cette procédure est appelé calibration.
Une question naturelle qui se pose est de savoir quels actifs choisir pour la calibration. Il existe plusieurs réponses possibles, en pratique on essaye de trouver un produit à la fois simple et liquide.

Dans notre cas il est indispensable que le modèle puissent retrouver les prix des bond zéro coupons.
Idéalement notre benchmarks est une ensemble de caplets. Les caplets ne sont pas tradés en tant que tel sur le marché, nous n'avons accès qu'à des caps. => Stripping

Le modèle à 2F permet de caputrer le hump de la courbe de rendement
Le nombre de paramètre est fini (5) => pas de overfitting


\begin{itemize}
\item $h(t)$ pour reconstruire la yield curve $\sigma \rho \nu$ pour
\item  matcher la surface
\end{itemize}

\subsection{Calibration du drift}
Le modèle gaussien à deux facteurs est calibré sur la
courbe P M (0, T ), T > 0 de prix d’obligations zéro-coupon observés sur le
marché si et seulement si $\phi$ est définie par :
$$\Phi = ...$$

Cependant, l’arbre ainsi simulé ne redonnera par exactement les prix des
obligations zéro-coupon P (0, T i ). En effet, dans un arbre le taux simulé est
considéré constant sur la période $[T_i, T_{i+1}$, donc tout se passe comme
si l’arbre simulait en fait un taux zéro-coupon R. En d’autres termes, le prix
d’une obligation zéro-coupon de maturité $T_1$ vaut
$P(0, T_1 ) = e^{-R(0, T_1) \Delta t}$
mais ce prix calculé directement sur l’arbre s’écrira $e^{-r_0\Delta t}$ .
On propose donc ici de calibrer récursivement les $\phi_i$ afin de
retrouver les prix des obligations zéro-coupon directement sur l’arbre. 
$\Pi_j$ state price (arrow debrew) (paye 1 si le noeud $(t_n, j)$ est atteint.

$$D_j(h_n) := \frac{1}{1 + r(t_n)(t_{n+1} - t_n)} $$ Le discount factor
$$ \Pi_j(t_{n+1}) = \sum_j \Pi_j(t_n) p_{j, j'}(t_n) D_{j'}(h_n)$$
$$ \sum \Pi_j D_j(h_n) = P(0, t_{n+1}) $$

Developement de taylor => trouver $h_n$

\subsection{Méthode de calibration de la surface - Méthode d'optimisation}

Les traders préfère aiment bien avoir un controle fin sur le modèle qui reflète leur sensation sur le marché. Pour celà, le modèle doit êtres paramètrble, les paramètres doivent avoir un sens/être compris par les traders.

Au lieu d'avoir un paramète par maturité, G2++ il y a 5 paramètres


\begin{itemize}
\item On s'autorise un intervalle pour les paramètres
\item On utilise une grille (définie par le pas) pour définir les valeurs
  autorisées pour chaque paramètre. (Tradeoff entre pas petite grande
  précision et temps de calculs)
\item On calcule le prix des caplets associés
\item On choisit les paramètre qui reflètent le mieux les prix du
  marché.
\end{itemize}

Maintenant que nous connaissons la dynamique de $P(t, T)$, nous pouvons fournir une formule explicite pour le prix des caplets.

En effet, sous la mesure forward neutre $Q_T$, l'equation vérifiér  par $(x, y)$ est:

\begin{align*}
  \rm{d}x &= -\alpha x(t) \rm{d}t - Drift_x + \sigma \rm{d}W^1_t \\
  \rm{d}y &= -\beta y(t) \rm{d}t - Drift_y + \nu \rm{d}W^2_t \\
\end{align*}

qui a donc pour solution:
\begin{align*}
  x(t) &= -\alpha x(t) \rm{d}t - Drift_x + \sigma \rm{d}W^1_t \\
  y(t) &= -\beta y(t) \rm{d}t - Drift_y + \nu \rm{d}W^2_t \\
\end{align*}

Nous rappeleons l'expression du caplet sour la mesure $Q_T$
$$ZBP = \Qespr{Q_T}{ ... } $$

$P(t, T)$ admet une distribution normale sous $Q_T$ conditionnellement à $F_t$, 
Le prix théorique:

$$ZBC = P(t, T) N( d_1 ) - P(t, T) K N(d_2)$$
$$d_{1/2} = \frac{ln \frac{P(t, S)}{KP(t, T)}}{V_p} +- \frac{1}{2} V_P $$

On a 5 paramètres à optimiser
On calibre les caplets

Les caplets ne sont pas directement disponilbe sur les marche
On calibre les swaptions/caps

\begin{itemize}
\item Le probleme de calbration est un probleme d'optimisation Courbe
  de vol implicite :
\item minimisation de l'erreur L2
\end{itemize}
\IMG{img/capsurf.png}{Cap surface}{0.5}

\subsubsection{Un mot sur le multi threading - gpu?}
Le modèle permet de 
multi process

%\input{annexe.tex}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
